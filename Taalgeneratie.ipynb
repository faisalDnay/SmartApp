{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisalDnay/SmartApp/blob/master/Taalgeneratie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opdracht 2.1: Taalgeneratie\n",
        "We gaan aan de slag met een Large Language Model van OpenAI van vóór ChatGPT: GPT2.0 (nou ja, een [Nederlandse afgeleide](https://huggingface.co/GroNLP/gpt2-small-dutch) dan).\n",
        "\n",
        "- Het voordeel aan dit model is dat het klein genoeg is dat het overal wel draait.\n",
        "- Het nadeel is dat het nogal slecht is. (OpenAI bracht niet voor niets pas de opvolger GPT3.0 commercieel uit).\n",
        "- Toch bestaat het model uit 129.000.000 parameters, deze parameters staan vast.\n",
        "\n",
        "Daarom gaan we met andere eigenschappen van het model aan de slag:\n",
        "\n",
        "- We beginnen natuurlijk met de tekstuele input, oftewel de prompt.\n",
        "- Daarna kijken we naar het aantal woorddelen (tokens in het Engels) dat de LLM terug mag geven.\n",
        "- Daarna kijken we naar de temperatuur van het model. Dit is gerelateerd aan de waarschijnlijkheid dat het model andere woorden selecteert dan het best voorspelde woorddeel/token.\n",
        "- Daarna kijken we naar het aantal alternatieve woorddelen dat het model overweegt.\n",
        "- En uiteindelijk kijken we naar de straf dat het model geeft voor het herhalen van woorddelen.\n",
        "\n",
        "**Het doel van deze opdracht is om de eigenschappen van het model zo in te stellen dat deze het antwoord geven dat we zoeken.**\n",
        "\n",
        "## 1. Installatie\n",
        "Weer hebben we als eerst een aantal modules nodig die geïnstalleerd moeten worden."
      ],
      "metadata": {
        "id": "5-0BkRr6m23X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X6BXHVGaYB_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.1. Installeer modules\n",
        "!pip install -U bitsandbytes transformers accelerate"
      ],
      "metadata": {
        "id": "V-iKERpjj4SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Modules importeren\n",
        "Ook nu moeten er wat modules geïmporteerd worden."
      ],
      "metadata": {
        "id": "6sX8o3yAoAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.1 Importeer modules\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wadVA6rIjovp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Functie definieren\n",
        "En we moeten dit keer één functie definieren, namelijk de functie die de LLM uitvoert. Ook nu is het niet nodig om deze code te begrijpen voor de oefening. Je mag het natuurlijk wel bekijken en proberen te begrijpen."
      ],
      "metadata": {
        "id": "1a11LfOgoUs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.1 Definieer LLM-functie\n",
        "\n",
        "def run_llm(prompt, max_new_tokens, temperature, top_k, repetition_penalty):\n",
        "    model_name = \"GroNLP/gpt2-small-dutch\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        output_scores=True,\n",
        "        return_dict_in_generate=True,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_tokens = outputs.sequences[0]\n",
        "    scores = outputs.scores\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    generated_text = tokenizer.decode(generated_tokens[len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "    print(\"Answer:\", generated_text)\n",
        "    print()\n",
        "\n",
        "    for i, logits in enumerate(scores):\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        topk_probs, topk_ids = torch.topk(probs, top_k, dim=-1)\n",
        "        topk_probs = topk_probs[0].tolist()\n",
        "        topk_ids = topk_ids[0].tolist()\n",
        "\n",
        "        token_id = generated_tokens[len(inputs.input_ids[0]) + i].item()\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "        token_prob = probs[0, token_id].item()\n",
        "\n",
        "        print(f\"Step {i+1}: Generated token: {token_str!r} (prob={token_prob:.4f})\")\n",
        "        print(f\"Top {top_k} candidates:\")\n",
        "        for rank, (tok_id, prob) in enumerate(zip(topk_ids, topk_probs), start=1):\n",
        "            tok_str = tokenizer.decode([tok_id])\n",
        "            marker = \"<-- chosen\" if tok_id == token_id else \"\"\n",
        "            print(f\"  {rank}. Token: {tok_str!r}, Probability: {prob:.4f} {marker}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "75urQzuZoMBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Instellen en uitvoeren van het model\n",
        "\n",
        "Hieronder zijn de eigenschappen van het model alvast ingesteld. De prompt van het model is \"Wat is de hoofdstad van Nederland?\"\n",
        "\n",
        "Het doel van deze opdracht is om de eigenschappen zo in te stellen dat we het antwoord krijgen \"De hoofdstad van Nederland is Amsterdam.\"\n",
        "\n",
        "Speel met de eigenschappen, en probeer te achterhalen wat ze precies voor invloed hebben op het antwoord.\n",
        "\n",
        "Probeer, wanneer je de juiste eigenschappen hebt gevonden, te begrijpen waarom dit inderdaad de juiste eigenschappen zijn en waarom het model doet deze doet.\n",
        "\n",
        "Tips:\n",
        "- max_new_tokens bepaalt hoeveel tokens het model terug mag geven. Hoeveel tokens heb je nodig?\n",
        "- temperature bepaalt hoe waarschijnlijk het is dat het model een ander token selecteert dan het best voorspelde token. Een hogere temperature geeft dus meer willekeur en daarmee creativiteit aan het antwoord (tussen 0 en 1, exclusief 0).\n",
        "- top_tokens geeft aan hoeveel tokens het model overweegt om het volgende token uit te kiezen.\n",
        "- repetition_penalty bepaalt de straf die het model geeft aan het herhalen van tokens. Een hogere penalty zorgt dus voor minder herhaling (tussen 0 en 1, exclusief 0)."
      ],
      "metadata": {
        "id": "B8M63U91R_04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Wat is de hoofdstad van Nederland?\\n\"\n",
        "max_new_tokens = 10\n",
        "temperature = 1\n",
        "top_tokens = 10\n",
        "repetition_penalty = 0.5\n",
        "\n",
        "run_llm(prompt, max_new_tokens, temperature, top_tokens, repetition_penalty)"
      ],
      "metadata": {
        "id": "omCXmFXAE2Wc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}