{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisalDnay/SmartApp/blob/master/Taalgeneratie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opdracht 2.1: Taalgeneratie\n",
        "We gaan aan de slag met een Large Language Model van OpenAI van vóór ChatGPT: GPT2.0 (nou ja, een [Nederlandse afgeleide](https://huggingface.co/GroNLP/gpt2-small-dutch) dan).\n",
        "\n",
        "- Het voordeel aan dit model is dat het klein genoeg is dat het overal wel draait.\n",
        "- Het nadeel is dat het nogal slecht is. (OpenAI bracht niet voor niets pas de opvolger GPT3.0 commercieel uit).\n",
        "- Toch bestaat het model uit 129.000.000 parameters, deze parameters staan vast.\n",
        "\n",
        "Daarom gaan we met andere eigenschappen van het model aan de slag:\n",
        "\n",
        "- We beginnen natuurlijk met de tekstuele input, oftewel de prompt.\n",
        "- Daarna kijken we naar het aantal woorddelen (tokens in het Engels) dat de LLM terug mag geven.\n",
        "- Daarna kijken we naar de temperatuur van het model. Dit is gerelateerd aan de waarschijnlijkheid dat het model andere woorden selecteert dan het best voorspelde woorddeel/token.\n",
        "- Daarna kijken we naar het aantal alternatieve woorddelen dat het model overweegt.\n",
        "- En uiteindelijk kijken we naar de straf dat het model geeft voor het herhalen van woorddelen.\n",
        "\n",
        "**Het doel van deze opdracht is om de eigenschappen van het model zo in te stellen dat deze het antwoord geven dat we zoeken.**\n",
        "\n",
        "## 1. Installatie\n",
        "Weer hebben we als eerst een aantal modules nodig die geïnstalleerd moeten worden."
      ],
      "metadata": {
        "id": "5-0BkRr6m23X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X6BXHVGaYB_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.1. Installeer modules\n",
        "!pip install -U bitsandbytes transformers accelerate"
      ],
      "metadata": {
        "id": "V-iKERpjj4SH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b56b289-eb2c-46da-a473-bbdb0ee54edd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, bitsandbytes\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed bitsandbytes-0.48.1 transformers-4.57.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Modules importeren\n",
        "Ook nu moeten er wat modules geïmporteerd worden."
      ],
      "metadata": {
        "id": "6sX8o3yAoAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.1 Importeer modules\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wadVA6rIjovp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Functie definieren\n",
        "En we moeten dit keer één functie definieren, namelijk de functie die de LLM uitvoert. Ook nu is het niet nodig om deze code te begrijpen voor de oefening. Je mag het natuurlijk wel bekijken en proberen te begrijpen."
      ],
      "metadata": {
        "id": "1a11LfOgoUs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.1 Definieer LLM-functie\n",
        "\n",
        "def run_llm(prompt, max_new_tokens, temperature, top_k, repetition_penalty):\n",
        "    model_name = \"GroNLP/gpt2-small-dutch\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        output_scores=True,\n",
        "        return_dict_in_generate=True,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_tokens = outputs.sequences[0]\n",
        "    scores = outputs.scores\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    generated_text = tokenizer.decode(generated_tokens[len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "    print(\"Answer:\", generated_text)\n",
        "    print()\n",
        "\n",
        "    for i, logits in enumerate(scores):\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        topk_probs, topk_ids = torch.topk(probs, top_k, dim=-1)\n",
        "        topk_probs = topk_probs[0].tolist()\n",
        "        topk_ids = topk_ids[0].tolist()\n",
        "\n",
        "        token_id = generated_tokens[len(inputs.input_ids[0]) + i].item()\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "        token_prob = probs[0, token_id].item()\n",
        "\n",
        "        print(f\"Step {i+1}: Generated token: {token_str!r} (prob={token_prob:.4f})\")\n",
        "        print(f\"Top {top_k} candidates:\")\n",
        "        for rank, (tok_id, prob) in enumerate(zip(topk_ids, topk_probs), start=1):\n",
        "            tok_str = tokenizer.decode([tok_id])\n",
        "            marker = \"<-- chosen\" if tok_id == token_id else \"\"\n",
        "            print(f\"  {rank}. Token: {tok_str!r}, Probability: {prob:.4f} {marker}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "75urQzuZoMBt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Instellen en uitvoeren van het model\n",
        "\n",
        "Hieronder zijn de eigenschappen van het model alvast ingesteld. De prompt van het model is \"Wat is de hoofdstad van Nederland?\"\n",
        "\n",
        "Het doel van deze opdracht is om de eigenschappen zo in te stellen dat we het antwoord krijgen \"De hoofdstad van Nederland is Amsterdam.\"\n",
        "\n",
        "Speel met de eigenschappen, en probeer te achterhalen wat ze precies voor invloed hebben op het antwoord.\n",
        "\n",
        "Probeer, wanneer je de juiste eigenschappen hebt gevonden, te begrijpen waarom dit inderdaad de juiste eigenschappen zijn en waarom het model doet deze doet.\n",
        "\n",
        "Tips:\n",
        "- max_new_tokens bepaalt hoeveel tokens het model terug mag geven. Hoeveel tokens heb je nodig?\n",
        "- temperature bepaalt hoe waarschijnlijk het is dat het model een ander token selecteert dan het best voorspelde token. Een hogere temperature geeft dus meer willekeur en daarmee creativiteit aan het antwoord (tussen 0 en 1, exclusief 0).\n",
        "- top_tokens geeft aan hoeveel tokens het model overweegt om het volgende token uit te kiezen.\n",
        "- repetition_penalty bepaalt de straf die het model geeft aan het herhalen van tokens. Een hogere penalty zorgt dus voor minder herhaling (tussen 0 en 1, exclusief 0)."
      ],
      "metadata": {
        "id": "B8M63U91R_04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Wat is de hoofdstad van Nederland?\\n\"\n",
        "max_new_tokens = 7\n",
        "temperature = 0.3\n",
        "top_tokens = 3\n",
        "repetition_penalty = 1.0\n",
        "\n",
        "run_llm(prompt, max_new_tokens, temperature, top_tokens, repetition_penalty)"
      ],
      "metadata": {
        "id": "omCXmFXAE2Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab247d66-06e7-4bb5-a290-b601674fd776"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Wat is de hoofdstad van Nederland?\n",
            "\n",
            "Answer: De hoofdstad van Nederland is Amsterdam.\n",
            "\n",
            "Step 1: Generated token: 'De' (prob=0.7724)\n",
            "Top 3 candidates:\n",
            "  1. Token: 'De', Probability: 0.7724 <-- chosen\n",
            "  2. Token: \"'\", Probability: 0.1385 \n",
            "  3. Token: '\"', Probability: 0.0891 \n",
            "\n",
            "Step 2: Generated token: ' hoofdstad' (prob=0.9980)\n",
            "Top 3 candidates:\n",
            "  1. Token: ' hoofdstad', Probability: 0.9980 <-- chosen\n",
            "  2. Token: ' stad', Probability: 0.0020 \n",
            "  3. Token: ' Nederlandse', Probability: 0.0000 \n",
            "\n",
            "Step 3: Generated token: ' van' (prob=0.9960)\n",
            "Top 3 candidates:\n",
            "  1. Token: ' van', Probability: 0.9960 <-- chosen\n",
            "  2. Token: ' is', Probability: 0.0040 \n",
            "  3. Token: ',', Probability: 0.0000 \n",
            "\n",
            "Step 4: Generated token: ' Nederland' (prob=0.9999)\n",
            "Top 3 candidates:\n",
            "  1. Token: ' Nederland', Probability: 0.9999 <-- chosen\n",
            "  2. Token: ' de', Probability: 0.0001 \n",
            "  3. Token: ' het', Probability: 0.0000 \n",
            "\n",
            "Step 5: Generated token: ' is' (prob=0.9993)\n",
            "Top 3 candidates:\n",
            "  1. Token: ' is', Probability: 0.9993 <-- chosen\n",
            "  2. Token: ',', Probability: 0.0005 \n",
            "  3. Token: '.', Probability: 0.0002 \n",
            "\n",
            "Step 6: Generated token: ' Amsterdam' (prob=0.4672)\n",
            "Top 3 candidates:\n",
            "  1. Token: ' Amsterdam', Probability: 0.4672 <-- chosen\n",
            "  2. Token: ' de', Probability: 0.3379 \n",
            "  3. Token: ' het', Probability: 0.1949 \n",
            "\n",
            "Step 7: Generated token: '.' (prob=0.9131)\n",
            "Top 3 candidates:\n",
            "  1. Token: '.', Probability: 0.9131 <-- chosen\n",
            "  2. Token: ',', Probability: 0.0868 \n",
            "  3. Token: ' en', Probability: 0.0000 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}